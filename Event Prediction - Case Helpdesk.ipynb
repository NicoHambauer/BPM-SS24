{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Einführung in Predictive Business Process Monitoring\n",
    "\n",
    "## Definition + Ziel\n",
    "Predictive Business Process Monitoring (PBPM) beinhaltet die Anwendung von Data-Mining- und Predictive-Analytics Techniken auf **Eventlogs**.\n",
    "\n",
    "Eventlogs entstehen durch das automatische Protokollieren von Aktivitäten, Zeitstempeln und Attributen in Informationssystemen und sind die Grundlage für die Analyse und Optimierung von Geschäftsprozessen.\n",
    "\n",
    "Ziel von PBPM ist die Vorhersage zukünftiger Eigenschaften laufender Prozesse durch Analyse und Modellierung von Mustern und Sequenzen in historischen Daten.\n",
    "\n",
    "\n",
    "## Strukture der Daten\n",
    "\n",
    "In einem typischen Eventlog findet man zumindest eine **case_id**, eine **activity** und einen **timestamp**. Zusätzlich sind öfter auch weitere **context** attribute zu finden.\n",
    "\n",
    "Prozessdaten können in einer Tabelle dargestellt werden, unterscheiden sich jedoch in der Representation von tabellarischen Daten.\n",
    "\n",
    "Erkennbar ist, dass im Gegensatz zu tabellarischen Daten, die in Zeilen (pro Zeile ein Sample) und Spalten (Features) organisiert sind, Eventlogs in Form von **Traces** oder auch Instanzen vorliegen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definition eines Samples und Features in einem Eventlog\n",
    "\n",
    "Hier gehören also mehrere Zeilen zu einem Sample. Ein Trace ist eine Sequenz von Ereignissen,\n",
    "welches die Aktivitäten eines Prozesses in der Reihenfolge ihres Auftretens darstellt.\n",
    "\n",
    "Das bedeutet, ein Sample besteht nicht aus einer Zeile, sondern aus einer Sequenz von Aktivitäten, die in einer bestimmten Reihenfolge stattgefunden haben.\n",
    "\n",
    "## Abtrennung von Prozess Instanzen durch eine Instanz ID / Case ID\n",
    "\n",
    "Markiert wird ein Trace oft durch eine **case_id**.\n",
    "Unsere Aufgabe ist es also, dem Modell einen (Teil) Trace zu geben und es soll uns sagen, was als nächstes passiert.\n",
    "\n",
    "Das Label ist also die nächste Aktivität, die in dem Trace passiert oder auch die Zeit, die bis zum Abschluss des Prozesses verbleibt.\n",
    "\n",
    "Es wird deutlich, dass einige Vorverarbeitung notwendig ist, um die Daten in ein Format zu bringen, das für Machine Learning Modelle geeignet ist."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"event log example.png\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wiederholung Process Mining\n",
    "\n",
    "Die Übersetzung in ein mögliches Prozessmodell in Form eines Graphs (hier links) sieht in dem Auszug relativ simpel und per Hand machbar aus.\n",
    "\n",
    "Sobald die Anzahl der Ereignisse und die Anzahl der Traces (Instanzen des Prozess) steigt, wird es jedoch schwierig, die Beziehungen zwischen den Ereignissen zu erkennen und zu modellieren. Ab einem gewissen Punkt brauchen wir Abstraktionen und Vereinfachung. Oft werden diese in verschiedenen Stufen durch verschiedene Algorithmen erreicht."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Existierende Klassifikations und Regressions Probleme in PBPM\n",
    "\n",
    "PBPM zielt darauf ab, Folgendes vorherzusagen:\n",
    "\n",
    "* nächste Aktivitäten an einem bestimmten Punkt in einem Prozess\n",
    "* (z.B. welche Aktivität wird als nächstes ausgeführt).\n",
    "* den Ausgang von Fällen (positiv / negativ, z.B. Annahme eines Kredits oder Ablehnung, ...).\n",
    "* die verbleibende Zeit bis zum Abschluss eines Falles (z.B. wie lange wird ein Kunde noch warten müssen).\n",
    "* andere relevante Fallattribute (z.B. die Wahrscheinlichkeit, dass ein Fall in einer bestimmten Abteilung bearbeitet wird).\n",
    "\n",
    "Diese Vorhersagen können die betriebliche Effizienz erheblich steigern, die Kundenzufriedenheit verbessern und die proaktive Entscheidungsfindung in verschiedenen Geschäftsprozessen erleichtern. Bis dahin ist es aber oft ein weiter Weg und Firmen müssen sich mit den Herausforderungen der Datenqualität, der Modellierung und der Integration in bestehende Systeme auseinandersetzen.\n",
    "\n",
    "Oft werden Tools wie von Celonis oder PM4PY angewandt, da diese speziell für die Prozessanalyse und -optimierung entwickelt wurden.\n",
    "Diese Tools inkludieren oft aber nur Analyse Werkzeuge.\n",
    "\n",
    "Für die Vorhersage von Prozessen werden oft Machine Learning Modelle verwendet, die in Python oder R entwickelt werden und Programmierkenntnisse voraussetzen.\n",
    "\n",
    "Historisch gibt es einige Low-Code Platformen oder Pakete, die uns aber nur bedingt beim Verständnis und der Anwendung von Machine Learning Modellen helfen.\n",
    "\n",
    "Im Folgenden wollen wir verstehen, wie wir mit einfachen Machine Learning Modellen in Python Eigenschaften von laufenden Prozesen vorhersagen können."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fallbeispiel + Daten Exploration\n",
    "### HelpDesk Dataset\n",
    "\n",
    "* Das Eventlog \"Helpdesk\" stammt aus dem Ticketing-Management-System des Helpdesks (Kundensupports) eines italienischen Softwareunternehmens.\n",
    "* Dieses Prozessprotokoll ist die Grundlage für die Untersuchung des Lebenszyklus von Helpdesk-Tickets, von der Erstellung bis zur Lösung.\n",
    "* Der Prozess, den wir aus diesem Eventlog ableiten können, besteht aus 14 verschiedenen Aktivitäten. Der Datensatz enthält 4580 Instanzen und 13.710 Ereignisse, mit einer Länge von 1 bis 14 Aktivitäten.\n",
    "* Die Analyse dieses Protokolls hilft dabei, den Ticketprozess zu verstehen und zu verbessern, um die Effizienz und Kundenzufriedenheit im Helpdesk-Betrieb zu steigern.\n",
    "* Die Daten zu diesem öffentlichen Datensatz finden Sie [hier](https://data.4tu.nl/articles/dataset/Dataset_belonging_to_the_help_desk_log_of_an_Italian_Company/12675977\n",
    "* Der Zeitstempel hilft zunächst bei der Analyse der Dauer der einzelnen Aktivitäten und des gesamten Prozesses. Ein Machine Learning modelle kann dann auf Basis dieser Daten trainiert werden, um zukünftige Prozesse zu prognostizieren.\n",
    "\n",
    "### Traces und ihre Visualisierung\n",
    "- Auswahl einiger Traces und manuelle Überführung in einen Graphen\n",
    "- Diskussion über die Bedeutung der Reihenfolge von Events in einem Prozess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We start off our Coding with some simple imports of packages that we will need"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "import time\n",
    "\n",
    "# import datev event log data\n",
    "# which consists of a string with \"case;event;time;DOMAIN;SUBSYSTEM;MICROSUBSYSTEM;EVENTTAX_5;EVENTTAX_6;EVENTTAX_7\"\n",
    "# case event and time are relevant and\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "eventlog = pd.read_csv('data_converted.csv', delimiter=';')\n",
    "#eventlog = pd.read_csv('helpdesk_converted.csv', delimiter=';')\n",
    "eventlog = eventlog.drop(columns=['time'])\n",
    "\n",
    "#modify data (datanew = data + 1)for calculation of gaussian process later, because event 0 is going to be a filler/blank\n",
    "#LATER this would be reversed if a system in practical use cases would be needed\n",
    "column_names = eventlog.columns ##take columns of a typical event log except the case column and increment\n",
    "for i in range(1, len(column_names)):\n",
    "    eventlog[column_names[i]] += 1\n",
    "\n",
    "maxeventseq = (eventlog.groupby(['case']).count()['event']).max()\n",
    "\n",
    "#now Build sample arrays for each case: for each k events do this k - 1 times\n",
    "case_groups = eventlog.groupby('case')\n",
    "\n",
    "samples_to_map = [] #contains a sequence of all samples in the event log, each sample is a 2 dimensional array of rows of event, domains and is filled with the number 0 for each row missing\n",
    "labels = [] #1 dimensional array\n",
    "\n",
    "for case, group in case_groups:\n",
    "    for n in range(len(group) - 1): #create n - 1 samples in this group\n",
    "        sample = [] #[maxeventseq][len(group.columns)] at the end\n",
    "        for i in range(n + 1): #use i to current n amount of events\n",
    "            sample_row = []\n",
    "            for r in range(1, len(group.columns)):\n",
    "                sample_row.append(group.iloc[i, r])\n",
    "            sample.append(sample_row)\n",
    "        for i_fillUp in range(n + 1, maxeventseq): #fill n to < maxeventseq with blank/0 event  ###minus 1 in range because of the last max event seq is going to be the lable\n",
    "            sample_row = []\n",
    "            for r in range(1, len(group.columns)):\n",
    "                sample_row.append(0)\n",
    "            sample.append(sample_row)\n",
    "\n",
    "        samples_to_map.append(sample)\n",
    "        labels.append(group.iloc[n + 1, 1])\n",
    "\n",
    "#map the samples arra from 3d to 2d by concating every sample\n",
    "samples = [] #array of mapped samples\n",
    "for sample in samples_to_map:\n",
    "    mapped_sample = []\n",
    "    for sample_row in sample:\n",
    "        for sample_feature in sample_row:\n",
    "            mapped_sample.append(sample_feature)\n",
    "    samples.append(mapped_sample)\n",
    "\n",
    "samples_np = np.array(samples, dtype=int)\n",
    "labels_np = np.array(labels, dtype=int)\n",
    "\n",
    "trainsize = 0.70\n",
    "testsize = 0.30\n",
    "#Classification happens here\n",
    "Sample_train, Sample_test, feature_train, feature_test = train_test_split(samples_np,\n",
    "                                                                          labels_np,\n",
    "                                                                          train_size=trainsize,\n",
    "                                                                          test_size=testsize,\n",
    "\n",
    "                                                                          )\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#kernel = 1.0 * RBF(length_scale=1.0)## [1.0] ##kernel=kernel #ConstantKernel(1.0) * RBF(length_scale=1.0)\n",
    "#kernel = 0.75 * RBF([0.75])\n",
    "gpc = RandomForestClassifier().fit(Sample_train, feature_train) #RandomForestClassifier #DecisionTreeClassifier(max_depth=5) ##KNeighborsClassifier(3)\n",
    "#gpc = GaussianProcessClassifier().fit(Sample_train, feature_train) #, n_jobs=4\n",
    "print('gpc done')\n",
    "stop = time.time()\n",
    "duration = int(stop - start)\n",
    "print(duration)\n",
    "\n",
    "#print options\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "# calculate\n",
    "accuracy =gpc.score(Sample_test, feature_test) # gpc\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "gpc_predictions = gpc.predict(Sample_test) #gpc\n",
    "cm = confusion_matrix(feature_test, gpc_predictions)\n",
    "#print(cm)\n",
    "\n",
    "result_cf = pd.DataFrame(cm)\n",
    "result_ac_time = pd.DataFrame([[accuracy, duration, trainsize, testsize\n",
    "                                #, kernel.k1\n",
    "                                ]], columns=['accuracy', 'runtime_in_s', 'training', 'testing'\n",
    "                                        # , 'kernel'\n",
    "                                            ]\n",
    "                              )\n",
    "result_cf.to_csv('result_conf_m.csv', index=False, sep=';')\n",
    "result_ac_time.to_csv('result_accur_time.csv', index=False, sep=\";\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sometimes we want to set options for printing the output in our console to show the entire dataframe.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PM4Py is a process mining library in Python that offers a wide range of tools and algorithms for the analysis, discovery, and enhancement of processes. Process mining is a field that focuses on the analysis of event logs to discover, monitor, and improve real processes by extracting knowledge from event logs readily available in today's systems. PM4Py implements various algorithms and techniques for process discovery, conformance checking, and enhancement, catering to the needs of process mining tasks.\n",
    "\n",
    "The differences between the Directly Follows Graph (DFG) miner, the Alpha Miner, the Inductive Miner, and the Heuristics Miner are primarily in their approach to discovering process models from event logs, each with its strengths and limitations:\n",
    "\n",
    "Directly Follows Graph Miner: This miner constructs a graph where nodes represent activities, and edges represent the direct succession of one activity by another in the event log. It's a simple and intuitive model, highlighting the direct relationships between activities. However, it might not capture more complex dependencies and can result in cluttered graphs for processes with many activities.\n",
    "Alpha Miner: One of the earliest and most straightforward process discovery algorithms, the Alpha Miner, can construct a Petri net (a mathematical modeling language) from an event log. It identifies causal dependencies between activities based on their order of appearance. While effective for simple, well-structured processes, it struggles with more complex scenarios, such as loops of varying lengths and parallel executions.\n",
    "Inductive Miner: This miner is designed to handle a wider variety of process behaviors, including complex structures like loops and parallel activities. It works by recursively splitting the log into simpler parts until it can apply base cases to construct a process model. The Inductive Miner is particularly known for its balance between accuracy and simplicity, providing models that are both readable and close to the event log data. It also has a variant that can guarantee sound process models, which are free from deadlocks and other anomalies.\n",
    "Heuristics Miner: This algorithm focuses on discovering process models based on heuristics derived from the event log. It aims to balance between overfitting and underfitting the log data by considering frequencies of activity occurrences and their relations. The Heuristics Miner is particularly useful for dealing with noisy data or logs with incomplete information. It constructs a process model that reflects the most common behavior observed in the event log, while potentially ignoring rare or exceptional paths.\n",
    "\n",
    "Ab einem gewissen Punkt brauchen wir Abstraktionen und Vereinfachung. Oft werden diese in verschiedenen Stufen durch verschiedene Algorithmen erreicht.\n",
    "Beispiel dafür sind von PM4PY:\n",
    "* Directly Follows Graph Miner (DFG). Hier werden die direkten Beziehungen zwischen den Ereignissen in einem Graphen dargestellt. Die Knoten repräsentieren Aktivitäten und die Kanten repräsentieren die direkte Abfolge von Aktivitäten. Oft können Zyklische Abhängigkeiten und parallele Ausführungen nicht dargestellt werden.\n",
    "* Alpha Miner. Hier wird ein Petri Netz aus einem Ereignisprotokoll erstellt. Es identifiziert kausale Abhängigkeiten zwischen Aktivitäten basierend auf ihrer Reihenfolge des Auftretens. Dieser Algorithmus ist effektiv für einfache, gut strukturierte Prozesse, hat aber Schwierigkeiten mit komplexeren Szenarien, wie Schleifen unterschiedlicher Längen und parallelen Ausführungen.\n",
    "* Inductive Miner. Dieser Miner ist darauf ausgelegt, eine breitere Vielfalt von Prozessverhalten zu handhaben, einschließlich komplexer Strukturen wie Schleifen und parallelen Aktivitäten. Er arbeitet, indem er das Protokoll rekursiv in einfachere Teile aufteilt, bis er Basisszenarien anwenden kann, um ein Prozessmodell zu erstellen. Der Inductive Miner ist bekannt für sein Gleichgewicht zwischen Genauigkeit und Einfachheit und liefert Modelle, die sowohl lesbar als auch nahe an den Ereignisprotokolldaten sind.\n",
    "* Heuristics Miner. Dieser Algorithmus konzentriert sich auf die Entdeckung von Prozessmodellen auf der Grundlage von Heuristiken, die aus dem Ereignisprotokoll abgeleitet wurden. Er zielt darauf ab, zwischen Überanpassung und Unteranpassung der Protokolldaten zu balancieren, indem er die Häufigkeiten des Auftretens von Aktivitäten und deren Beziehungen berücksichtigt. Der Heuristics Miner ist besonders nützlich für den Umgang mit rauschhaften Daten oder Protokollen mit unvollständigen Informationen. Er erstellt ein Prozessmodell, das das häufigste Verhalten im Ereignisprotokoll widerspiegelt, wobei möglicherweise seltene oder außergewöhnliche Pfade ignoriert werden."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Projekt: Case 2 - BPI Challenge 2012 (BPI12 W Complete) Event Log\n",
    "\n",
    "The BPI Challenge 2012 event log details an application process for personal loans or overdrafts within a global financing organization, encompassing three subprocesses related to application states, work item states, and their interactions. To focus on manually executed activities, the dataset is pre-processed to exclude automatic events, resulting in 9,658 work item traces and 72,413 \"complete\" type events across 6 activities. This preprocessing, consistent with prior studies, allows for meaningful comparisons and insights into the manual aspects of the loan application process, with trace lengths varying between 1 and 74."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
